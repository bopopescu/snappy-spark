apply plugin: 'wrapper'

// TODO: profiles and allow changing hadoopVersion

buildscript {
  repositories {
    maven { url "https://plugins.gradle.org/m2" }
  }
  dependencies {
    classpath "com.github.maiflai:gradle-scalatest:0.10"
  }
}

allprojects {
  // We want to see all test results.  This is equivalatent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  repositories {
    mavenLocal()
    jcenter()
    maven { url "https://repository.apache.org/content/repositories/releases" }
    maven { url "https://repository.jboss.org/nexus/content/repositories/releases" }
    maven { url "https://repo.eclipse.org/content/repositories/paho-releases" }
    maven { url "https://repository.cloudera.com/artifactory/cloudera-repos" }
    maven { url "https://oss.sonatype.org/content/repositories/orgspark-project-1113" }
    maven { url "http://repository.mapr.com/maven" }
    maven { url "https://repo.spring.io/libs-release" }
    maven { url "http://maven.twttr.com" }
    maven { url "http://repository.apache.org/snapshots" }
  }

  apply plugin: 'scala'
  apply plugin: 'maven'

  // apply compiler options
  sourceCompatibility = 1.7
  targetCompatibility = 1.7

  compileJava.options.encoding = 'UTF-8'
  compileScala.options.encoding = 'UTF-8'
  compileJava.options.compilerArgs << '-Xlint:all,-serial,-path'

  group = 'org.apache.spark'
  version = '1.5.1-SNAPSHOT.1'

  ext {
    scalaBinaryVersion = '2.10'
    scalaVersion = scalaBinaryVersion + '.4'
    hadoopVersion = '2.4.1'
    jettyVersion = '8.1.14.v20131031'
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.12'
    junitVersion = '4.11'
    akkaVersion = '2.3.11'
    // the netty version from akka
    akkaNettyVersion = '3.8.0.Final'
    derbyVersion = '10.10.2.0'
  }

  // default output directory like in sbt/maven
  buildDir = 'build-artifacts/scala-' + scalaBinaryVersion

  ext {
    if (rootProject.name == 'snappy-spark') {
      subprojectBase = ':'
      sparkProjectRoot = ':'
      testResultsBase = "${rootProject.buildDir}/tests"
    } else {
      subprojectBase = ':snappy-spark:'
      sparkProjectRoot = ':snappy-spark'
      testResultsBase = "${rootProject.buildDir}/tests/spark"
    }
  }

  dependencies {
    compile 'org.scala-lang:scala-library:' + scalaVersion
    compile 'org.scala-lang:scala-reflect:' + scalaVersion
  }
}

def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

task cleanSparkScalaTest << {
  def workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanSparkJUnit << {
  def workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
}

subprojects {
  // when invoking from snappy-commons, below are already defined at top-level
  if (rootProject.name == 'snappy-spark') {
    task packageSources(type: Jar, dependsOn: classes) {
      classifier = 'sources'
      from sourceSets.main.allSource
    }
    task packageDocs(type: Jar, dependsOn: javadoc) {
      classifier = 'sources'
      from javadoc.destinationDir
    }
    /*
    artifacts {
      archives packageSources
      archives packageDocs
    }
    */

    configurations {
      testOutput {
        extendsFrom testCompile
        description 'a dependency that exposes test artifacts'
      }
    }

    task packageTests(type: Jar) {
      from sourceSets.test.output
      classifier = 'tests'
    }
    artifacts {
      testOutput packageTests
    }
  }

  // fix scala+java mix to all use compileScala which use correct dependency order
  sourceSets.main.scala.srcDir 'src/main/java'
  sourceSets.main.java.srcDirs = []

  dependencies {
    // This is a dummy dependency that is used along with the shading plug-in
    // to create effective poms on publishing (see SPARK-3812).
    //compile group: 'org.spark-project.spark', name: 'unused', version:'1.0.0'

    compile group: 'com.google.guava', name: 'guava', version: '14.0.1'
    compile group: 'log4j', name:'log4j', version: log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
    testCompile 'org.scalatest:scalatest_' + scalaBinaryVersion + ':2.2.1'
    testCompile 'org.mockito:mockito-core:1.9.5'
    testCompile 'org.scalacheck:scalacheck_' + scalaBinaryVersion + ':1.11.3'
    testCompile 'com.novocode:junit-interface:0.10'

    testRuntime 'org.pegdown:pegdown:1.1.0'
  }

  if (rootProject.name == 'snappy-spark') {
    task scalaTest(type: Test) {
      actions = [ new com.github.maiflai.ScalaTestAction() ]

      List<String> suites = []
      extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
      extensions.add("suite", { String name -> suites.add(name) } )
      extensions.add("suites", { String... name -> suites.addAll(name) } )

      // running a single scala suite
      if (rootProject.hasProperty('singleSuite')) {
        suite singleSuite
      }
    }
  }
  scalaTest {
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    systemProperties 'test.src.tables': '__not_used__'

    workingDir = "${testResultsBase}/scalatest"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test {
    jvmArgs '-Xss4096k'
    maxParallelForks = (2 * Runtime.getRuntime().availableProcessors())
    systemProperties 'spark.master.rest.enabled': 'false',
      'test.src.tables': 'src'

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  // need to do below after graph is ready else it will give an error about
  // runtimeClaspath being set after being finalized
  gradle.taskGraph.whenReady({ graph ->
    tasks.withType(Test).each { test ->
      test.configure {
        onlyIf { ! Boolean.getBoolean('skip.tests') }

        jvmArgs '-XX:+HeapDumpOnOutOfMemoryError', '-XX:MaxPermSize=512M', '-XX:ReservedCodeCacheSize=512m'
        maxHeapSize '3g'
        // disable assertions for hive tests as in Spark's pom.xml because HiveCompatibilitySuite currently fails (SPARK-4814)
        if (test.project.name.contains('snappy-spark-hive_')) {
          jvmArgs '-da'
          maxParallelForks = 1
        } else {
          jvmArgs '-ea'
        }
        environment 'SPARK_DIST_CLASSPATH': "${sourceSets.test.runtimeClasspath.asPath}",
          'SPARK_PREPEND_CLASSES': '1',
          'SPARK_TESTING': '1',
          'SPARK_SCALA_VERSION': scalaBinaryVersion,
          'JAVA_HOME': System.getProperty('java.home')
        systemProperties 'derby.system.durability': 'test',
          'java.awt.headless': 'true',
          'java.io.tmpdir': "${rootProject.buildDir}/tmp",
          'spark.test.home': "${rootProject.buildDir}/snappy",
          'spark.project.home': "${project(sparkProjectRoot).projectDir}",
          'spark.testing': '1',
          'spark.ui.enabled': 'false',
          'spark.ui.showConsoleProgress': 'false',
          'spark.driver.allowMultipleContexts': 'true',
          'spark.unsafe.exceptionOnMemoryLeak': 'true'

        testLogging.exceptionFormat = 'full'

        if (rootProject.name == 'snappy-spark') {
          def eol = System.getProperty('line.separator')
          beforeTest { desc ->
            def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
            def progress = new File(workingDir, "progress.txt")
            def output = new File(workingDir, "output.txt")
            progress << "$now Starting test $desc.className $desc.name$eol"
            output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
          }
          onOutput { desc, event ->
            def output = new File(workingDir, "output.txt")
            output  << event.message
          }
          afterTest { desc, result ->
            def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
            def progress = new File(workingDir, "progress.txt")
            def output = new File(workingDir, "output.txt")
            progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
            output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
            result.exceptions.each { t ->
              progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
              output << "${getStackTrace(t)}${eol}"
            }
          }
        }
      }
    }
  })
  test.dependsOn subprojectBase + 'cleanSparkJUnit'
  scalaTest.dependsOn subprojectBase + 'cleanSparkScalaTest'
  check.dependsOn scalaTest
}

check.dependsOn subprojects.check
